{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "for key, value in [\n",
    "    (\"PROJECT_NAME\", 'my_autotrain_llm'),\n",
    "    (\"MODEL_NAME\", 'abhishek/llama-2-7b-hf-small-shards'),\n",
    "    (\"PUSH_TO_HUB\", False), \n",
    "    (\"LEARNING_RATE\", 2e-4),\n",
    "    (\"NUM_EPOCHS\", 1),\n",
    "    (\"BATCH_SIZE\", 1),\n",
    "    (\"BLOCK_SIZE\", 1024),\n",
    "    (\"WARMUP_RATIO\", 0.1),\n",
    "    (\"WEIGHT_DECAY\", 0.01),\n",
    "    (\"GRADIENT_ACCUMULATION\", 4),\n",
    "    (\"MIXED_PRECISION\", 'fp16'),\n",
    "    (\"PEFT\", True),\n",
    "    (\"QUANTIZATION\", 'int4'),\n",
    "    (\"LORA_R\", 16),\n",
    "    (\"LORA_ALPHA\", 32),\n",
    "    (\"LORA_DROPOUT\", 0.05)\n",
    "    ]:\n",
    "        if not os.environ.get(key):\n",
    "            os.environ[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start1\n",
      "Command: \n",
      "['autotrain', 'llm', '--train', '--model', 'abhishek/llama-2-7b-hf-small-shards', '--project-name', 'my_autotrain_llm', '--data-path', './../train/image_o/data/train.csv', '--text-column', 'text', '--lr', '0.0002', '--batch-size', '1', '--epochs', '1', '--block-size', '1024', '--warmup-ratio', '0.1', '--lora-r', '16', '--lora-alpha', '32', '--lora-dropout', '0.05', '--weight-decay', '0.01', '--gradient-accumulation', '4', '--quantization', 'int4', '--mixed-precision', 'fp16', '--PEFT']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommand: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(command)\n\u001b[1;32m---> 29\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m jsonify({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mstderr})\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "print('start1')\n",
    "command = [\n",
    "    \"autotrain\", \"llm\", \"--train\",\n",
    "    \"--model\", os.environ.get(\"MODEL_NAME\"),\n",
    "    \"--project-name\", os.environ.get(\"PROJECT_NAME\"),\n",
    "    \"--data-path\", \"./../train/image_o/data/train.csv\",\n",
    "    \"--text-column\", \"text\",\n",
    "    \"--lr\", os.environ.get(\"LEARNING_RATE\"),\n",
    "    \"--batch-size\", os.environ.get(\"BATCH_SIZE\"),\n",
    "    \"--epochs\", os.environ.get(\"NUM_EPOCHS\"),\n",
    "    \"--block-size\", os.environ.get(\"BLOCK_SIZE\"),\n",
    "    \"--warmup-ratio\", os.environ.get(\"WARMUP_RATIO\"),\n",
    "    \"--lora-r\", os.environ.get(\"LORA_R\"),\n",
    "    \"--lora-alpha\", os.environ.get(\"LORA_ALPHA\"),\n",
    "    \"--lora-dropout\", os.environ.get(\"LORA_DROPOUT\"),\n",
    "    \"--weight-decay\", os.environ.get(\"WEIGHT_DECAY\"),\n",
    "    \"--gradient-accumulation\", os.environ.get(\"GRADIENT_ACCUMULATION\"),\n",
    "    \"--quantization\", os.environ.get(\"QUANTIZATION\"),\n",
    "    \"--mixed-precision\", os.environ.get(\"MIXED_PRECISION\"),\n",
    "]\n",
    "\n",
    "# Conditional flags\n",
    "if os.environ.get(\"PEFT\") == \"True\": command.append(\"--PEFT\")\n",
    "if os.environ.get(\"PUSH_TO_HUB\") == \"True\": command.append(\"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\") \n",
    "\n",
    "# Execute the command\n",
    "print(\"Command: \")\n",
    "print(command)\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "jsonify({\"output\": result.stdout, \"error\": result.stderr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"AutoTrain advanced CLI\",\n",
    "        usage=\"autotrain <command> [<args>]\",\n",
    "        epilog=\"For more information about a command, run: `autotrain <command> --help`\",\n",
    "    )\n",
    "    parser.add_argument(\"--version\", \"-v\", help=\"Display AutoTrain version\", action=\"store_true\")\n",
    "    commands_parser = parser.add_subparsers(help=\"commands\")\n",
    "\n",
    "    # Register commands\n",
    "    RunAutoTrainAppCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainLLMCommand.register_subcommand(commands_parser)\n",
    "    RunSetupCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainDreamboothCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainAPICommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainTextClassificationCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainImageClassificationCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainTabularCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainSpaceRunnerCommand.register_subcommand(commands_parser)\n",
    "    RunAutoTrainSeq2SeqCommand.register_subcommand(commands_parser)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.version:\n",
    "        print(__version__)\n",
    "        exit(0)\n",
    "\n",
    "    if not hasattr(args, \"func\"):\n",
    "        parser.print_help()\n",
    "        exit(1)\n",
    "\n",
    "    command = args.func(args)\n",
    "    command.run()\n",
    "\n",
    "\n",
    "class RunAutoTrainLLMCommand(BaseAutoTrainCommand):\n",
    "    def run(self):\n",
    "        from autotrain.backend import EndpointsRunner, SpaceRunner\n",
    "        from autotrain.trainers.clm.__main__ import train as train_llm\n",
    "        from autotrain.trainers.clm.params import LLMTrainingParams\n",
    "\n",
    "        logger.info(\"Running LLM\")\n",
    "        logger.info(f\"Params: {self.args}\")\n",
    "        if self.args.train:\n",
    "            params = LLMTrainingParams(\n",
    "                model=self.args.model,\n",
    "                data_path=self.args.data_path,\n",
    "                train_split=self.args.train_split,\n",
    "                valid_split=self.args.valid_split,\n",
    "                text_column=self.args.text_column,\n",
    "                lr=self.args.learning_rate,\n",
    "                epochs=self.args.num_train_epochs,\n",
    "                batch_size=self.args.train_batch_size,\n",
    "                warmup_ratio=self.args.warmup_ratio,\n",
    "                gradient_accumulation=self.args.gradient_accumulation_steps,\n",
    "                optimizer=self.args.optimizer,\n",
    "                scheduler=self.args.scheduler,\n",
    "                weight_decay=self.args.weight_decay,\n",
    "                max_grad_norm=self.args.max_grad_norm,\n",
    "                seed=self.args.seed,\n",
    "                add_eos_token=self.args.add_eos_token,\n",
    "                block_size=self.args.block_size,\n",
    "                use_peft=self.args.use_peft,\n",
    "                lora_r=self.args.lora_r,\n",
    "                lora_alpha=self.args.lora_alpha,\n",
    "                lora_dropout=self.args.lora_dropout,\n",
    "                logging_steps=self.args.logging_steps,\n",
    "                project_name=self.args.project_name,\n",
    "                evaluation_strategy=self.args.evaluation_strategy,\n",
    "                save_total_limit=self.args.save_total_limit,\n",
    "                save_strategy=self.args.save_strategy,\n",
    "                auto_find_batch_size=self.args.auto_find_batch_size,\n",
    "                fp16=self.args.fp16,\n",
    "                push_to_hub=self.args.push_to_hub,\n",
    "                use_int8=self.args.use_int8,\n",
    "                model_max_length=self.args.model_max_length,\n",
    "                repo_id=self.args.repo_id,\n",
    "                use_int4=self.args.use_int4,\n",
    "                trainer=self.args.trainer,\n",
    "                target_modules=self.args.target_modules,\n",
    "                token=self.args.token,\n",
    "                merge_adapter=self.args.merge_adapter,\n",
    "                username=self.args.username,\n",
    "                use_flash_attention_2=self.args.use_flash_attention_2,\n",
    "                log=self.args.log,\n",
    "                rejected_text_column=self.args.rejected_text_column,\n",
    "                disable_gradient_checkpointing=self.args.disable_gradient_checkpointing,\n",
    "                model_ref=self.args.model_ref,\n",
    "                dpo_beta=self.args.dpo_beta,\n",
    "                prompt_text_column=self.args.prompt_text_column,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes to Michael - 5:08pm 11/7/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put notes in this notebook about auto-training using xturing. \n",
    "\n",
    "It covers generating datasets using a prepped file, auto-creating it using jsonl, or generating from files in a directory. The 'CHATGPT' api call is depricated in the library but if you want to fix it and get the 'generate from directory' example working that would be DOPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been working in a google colab enviornment connected to a cloud instance for training here:\n",
    "https://colab.research.google.com/drive/1hjUMbrJhS92a9tEzVKpgEZx8vU4QxBtx?usp=sharing\n",
    "\n",
    "The colab notebook is a cleaner notebook where I am also developing the docker/deploy stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface provides an autoTrain API that operates similarly. \n",
    "\n",
    "Weather or not we train our models using this api, is less of a concern at the moment. \n",
    "\n",
    "At the moment we really just want to create code which uses a users yaml doc to auto-deploy a localmodel to the huggingface hub and getting back an endpoint. For testing purposes the model does not have to be a trained and you can test deploying a base model from xturing demonstrated below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HF Autotrain notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoTrain Api has code for deploying models to huggingface hubs but the api docs are lacking.\n",
    "\n",
    "Here is a google colab of someone training using the service via CLI\n",
    "\n",
    "https://colab.research.google.com/drive/1ufB53v_ptm6NJYeemCgYAIGWGUIjO8yw#scrollTo=g3cd_ED_yXXt\n",
    "\n",
    "Forum:\n",
    "https://discuss.huggingface.co/c/autotrain/16\n",
    "\n",
    "- To let AutoTrain choose the best models for your task, you can use the “AutoTrain” in the “Model Choice” section. Once you choose AutoTrain mode, you no longer need to worry about model and parameter selection. AutoTrain will automatically select the best models (and parameters) for your task.\n",
    "\n",
    "\n",
    "Documentation:\n",
    "https://huggingface.co/docs/autotrain/llm_finetuning\n",
    "\n",
    "Useful things I found in the codebase:\n",
    "\n",
    "class AutoTrainDataset: \n",
    "https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/dataset.py\n",
    "\n",
    "autotrain api.py\n",
    "https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/api.py\n",
    "\n",
    "autotrain app.py\n",
    "https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/app.py\n",
    "\n",
    "autotrain config\n",
    "https://github.com/huggingface/autotrain-advanced/blob/main/src/autotrain/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package xturing.datasets in xturing:\n",
      "\n",
      "NAME\n",
      "    xturing.datasets\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    instruction_dataset\n",
      "    text2image_dataset\n",
      "    text_dataset\n",
      "\n",
      "FILE\n",
      "    /home/carlos/.local/lib/python3.10/site-packages/xturing/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xturing.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial LLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.models import BaseModel\n",
    "model = BaseModel.create('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(texts=['Hi How are you?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nI’m good how are you?\\nNot too bad. Just watching tv and eating a bowl of ice cream.\\nSounds good what are you watching?\\nA show called The Good Place. It's on Netflix if you want to check it out.\\nI have it on my netflix but I haven’t watched it yet. I should do that.\\nYeah it's good. I'm a fan of Parks and Rec so I was excited to see it on Netflix.\\nParks and Rec is my favorite show of all time. I watched it for the first time last year and fell in love with it.\\nSame here. The first time I watched it I was like wtf is going on? But I kept watching and it grew on me.\\nI think it's one of those shows that you need to watch multiple times to get the full experience.\\nDefinitely. I've rewatched it a few times and each time I notice something new that I didn't the first time.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights will be saved into 2 files. The whole model weights including based model parameters and LoRA parameters are stored in pytorch_model.bin file and only LoRA parameters are stored in adapter_model.bin file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert the alpaca JSON dataset to HF format\n",
    "\n",
    "# Right now only the HuggingFace datasets are supported, that's why the JSON Alpaca dataset\n",
    "# needs to be converted to the HuggingFace format. In addition, this HF dataset should have 3 columns for instruction finetuning: instruction, text and target.\n",
    "def preprocess_alpaca_json_data(alpaca_dataset_path: str):\n",
    "    \"\"\"Creates a dataset given the alpaca JSON dataset. You can download it here: https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
    "\n",
    "    :param alpaca_dataset_path: path of the Alpaca dataset\n",
    "    \"\"\"\n",
    "    alpaca_data = json.load(open(alpaca_dataset_path))\n",
    "    instructions = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for data in alpaca_data:\n",
    "        instructions.append(data[\"instruction\"])\n",
    "        inputs.append(data[\"input\"])\n",
    "        outputs.append(data[\"output\"])\n",
    "\n",
    "    data_dict = {\n",
    "        \"train\": {\"instruction\": instructions, \"text\": inputs, \"target\": outputs}\n",
    "    }\n",
    "\n",
    "    dataset = DatasetDict()\n",
    "    # using your `Dict` object\n",
    "    for k, v in data_dict.items():\n",
    "        dataset[k] = Dataset.from_dict(v)\n",
    "\n",
    "    dataset.save_to_disk(str(\"./data/alpaca_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba061a9ae3c49528d61d2703534ad06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_alpaca_json_data(\"./data/alpaca_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.datasets import InstructionDataset\n",
    "from xturing.model_apis.openai import ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogen from folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/stochasticai/xTuring/blob/55eda97e51e6b04c6796ae12104dd11cda362a47/examples/datasets/create_instruction_dataset_from_files.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some finance news articles are stored in sample_finance_data folder. In this tutorial, we are going to generate InstructionDataset from a data folder (finance news articles) and perform instruction fine-tuning on the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.datasets import InstructionDataset\n",
    "from xturing.model_apis.openai import ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = ChatGPT(\"your-api-key\")\n",
    "dataset = InstructionDataset.generate_dataset_from_dir(engine=engine, path=\"./sample_finance_data\")\n",
    "dataset.save(\"./output_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.models import BaseModel\n",
    "model = BaseModel.create(\"gpt2_lora\")\n",
    "# Finetune the model on generated dataset\n",
    "model.finetune(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autogen From Seed Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xturing.stochastic.ai/advanced/generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/seed_tasks.jsonl\n",
    "{\"id\": \"seed_task_0\", \"name\": \"breakfast_suggestion\", \"instruction\": \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\", \"instances\": [{\"input\": \"\", \"output\": \"Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.\"}], \"is_classification\": false}\n",
    "{\"id\": \"seed_task_1\", \"name\": \"antonym_relation\", \"instruction\": \"What is the relation between the given pairs?\", \"instances\": [{\"input\": \"Night : Day :: Right : Left\", \"output\": \"The relation between the given pairs is that they are opposites.\"}], \"is_classification\": false}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-06 22:53:26,156] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING: CUDA is not available, using CPU instead, can be very slow\n",
      "/home/carlos/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "from xturing.model_apis.openai import ChatGPT\n",
    "from xturing.datasets import InstructionDataset\n",
    "engine = ChatGPT(\"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\") ## Generate the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InstructionDataset.generate_dataset(engine=engine, path=\"./data/seed_tasks.jsonl\")\n",
    "dataset.save(\"./output_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.models import BaseModel\n",
    "model = BaseModel.create(\"gpt2_lora\")\n",
    "# Finetune the model on generated dataset\n",
    "model.finetune(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Llama Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.datasets.instruction_dataset import InstructionDataset\n",
    "from xturing.models import BaseModel\n",
    "\n",
    "instruction_dataset = InstructionDataset(\"./data/alpaca_data\")\n",
    "# Initializes the model\n",
    "model = BaseModel.create(\"llama\")\n",
    "# Finetuned the model\n",
    "model.finetune(dataset=instruction_dataset)\n",
    "# Once the model has been finetuned, you can start doing inferences\n",
    "output = model.generate(texts=[\"Why LLM models are becoming so important?\"])\n",
    "print(\"Generated output by the model: {}\".format(output))\n",
    "# Save the model\n",
    "model.save(\"./llama_weights\")\n",
    "\n",
    "# If you want to load the model just do BaseModel.load(\"./llama_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune Playground Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xturing.datasets import InstructionDataset\n",
    "from xturing.model_apis.openai import ChatGPT\n",
    "from xturing.datasets.instruction_dataset import InstructionDataset\n",
    "from xturing.models.base import BaseModel\n",
    "from xturing.ui.playground import Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n"
     ]
    }
   ],
   "source": [
    "# Initializes the model\n",
    "model = BaseModel.create(\"gpt2_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameThingAs = \"\"\"\n",
    "    InstructionDataset({\n",
    "        \"text\": [\"first text\", \"second text\"],\n",
    "        \"target\": [\"first text\", \"second text\"],\n",
    "        \"instruction\": [\"first instruction\", \"second instruction\"]\n",
    "    })\n",
    "    \"\"\"\n",
    "instruction_dataset = InstructionDataset(\"./data/alpaca_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_config = model.finetuning_config()\n",
    "finetuning_config.batch_size = 64 # 16\n",
    "finetuning_config.num_train_epochs = 1 # 3\n",
    "finetuning_config.learning_rate = 1e-5 # 0.003\n",
    "finetuning_config.weight_decay = 0.01\n",
    "finetuning_config.optimizer_name = \"adamw\"\n",
    "finetuning_config.output_dir = \"./data/training_dir/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xturing.stochastic.ai/configuration/finetune_configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinetuningConfig(learning_rate=1e-05, gradient_accumulation_steps=1, batch_size=64, weight_decay=0.01, warmup_steps=50, eval_steps=5000, save_steps=5000, max_length=512, num_train_epochs=1, logging_steps=10, max_grad_norm=2.0, save_total_limit=4, optimizer_name='adamw', output_dir='./data/training_dir/')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 124734720 || trainable%: 0.23643136409814364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718fd19e017a4348850ccfb236efb15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Finetuned the model\n",
    "model.finetune(dataset=instruction_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "model_path = \"./gpt2_weights\"\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "# launch the playground\n",
    "Playground(model_path).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Fine Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT AutoGen Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORR\n",
      "Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Hello! It seems like you're trying to start a conversation, but I'm not sure what you're looking for. Can you please provide more information or ask a specific question?\", role='assistant', function_call=None, tool_calls=None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Hello! It seems like you're trying to start a conversation, but I'm not sure what you're looking for. Can you please provide more information or ask a specific question?\", role='assistant', function_call=None, tool_calls=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.get_completion(\"How do you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.OpenAI().chat.completions.create(\"How do you do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8I7SlKlDwi2KViuEeKICoEZ76Lgmy', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The World Series in 2020 was played at the Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1699329283, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=53, total_tokens=72))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InstructionDataset.generate_dataset(path=\"./tasks.jsonl\", engine=engine)\n",
    "# dataset.save('testgptdataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xturing.stochastic.ai/overview/quickstart/prepare/#save-a-dataset\n",
    "\n",
    "InstructionDataset - You want the model to generate text based on an instruction/task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8095d092e5e4d61b4b3eec2e2c8a64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xturing.model_apis.openai import ChatGPT\n",
    "from xturing.datasets import InstructionDataset\n",
    "dataset = InstructionDataset({\n",
    "    \"text\": [\"first text\", \"second text\"],\n",
    "    \"target\": [\"first text\", \"second text\"],\n",
    "    \"instruction\": [\"first instruction\", \"second instruction\"]\n",
    "})\n",
    "dataset.save('testinstructdataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Versioning Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "# program\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.6\n"
     ]
    }
   ],
   "source": [
    "# program\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.6\n"
     ]
    }
   ],
   "source": [
    "# windows kernal\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.6\n"
     ]
    }
   ],
   "source": [
    "# windows kernal\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: python: command not found\n"
     ]
    }
   ],
   "source": [
    "# WSL\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "# WSL\n",
    "!python3 --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
