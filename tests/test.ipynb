{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/tatsu-lab/alpaca?row=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#hide\n",
    "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def preprocess_alpaca_json_to_text_array(alpaca_dataset_path: str):\n",
    "    \"\"\"\n",
    "    Creates an array of text representations given the Alpaca JSON dataset.\n",
    "\n",
    "    :param alpaca_dataset_path: path of the Alpaca dataset\n",
    "    :return: array of formatted text instructions with input and response\n",
    "    \"\"\"\n",
    "    with open(alpaca_dataset_path, 'r') as f:\n",
    "        alpaca_data = json.load(f)\n",
    "\n",
    "    text_array = []\n",
    "\n",
    "    for data in alpaca_data:\n",
    "        instruction = f\"### Instruction: {data['instruction']}\\n\"\n",
    "        input_text = f\"### Input: {data['input']}\\n\"\n",
    "        response = f\"### Response: {data['output']}\\n\"\n",
    "        formatted_text = f\"{instruction}\\n{input_text}\\n{response}\\n\"\n",
    "        text_array.append(formatted_text)\n",
    "\n",
    "    return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_csv_from_text_array(text_array, csv_output_path: str):\n",
    "    \"\"\"\n",
    "    Saves an array of text representations to a CSV file with a single 'text' column.\n",
    "\n",
    "    :param text_array: array of formatted text instructions with input and response\n",
    "    :param csv_output_path: path to save the CSV file\n",
    "    \"\"\"\n",
    "    with open(csv_output_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['text'])  # Write the header\n",
    "        for text_entry in text_array:\n",
    "            writer.writerow([text_entry])  # Write each text entry as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: What are the three primary colors?\n",
      "\n",
      "### Input: \n",
      "\n",
      "### Response: The three primary colors are red, blue, and yellow.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "alpaca_dataset_path = \"alpaca_data.json\"\n",
    "text_array = preprocess_alpaca_json_to_text_array(alpaca_dataset_path)\n",
    "\n",
    "# Now, 'formatted_data_array' contains the formatted strings.\n",
    "# If you want to see the first element to verify:\n",
    "print(text_array[1])\n",
    "\n",
    "create_csv_from_text_array(text_array, 'test.csv')  # Save the array to 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def preprocess_alpaca_json_to_csv(alpaca_dataset_path: str, csv_output_path: str):\n",
    "    \"\"\"\n",
    "    Creates a CSV file from the Alpaca JSON dataset with a single 'text' column.\n",
    "\n",
    "    :param alpaca_dataset_path: path of the Alpaca dataset\n",
    "    :param csv_output_path: path to save the CSV file\n",
    "    \"\"\"\n",
    "    with open(alpaca_dataset_path, 'r') as f:\n",
    "        alpaca_data = json.load(f)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_output_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['text'])\n",
    "\n",
    "        # Write the formatted text to the CSV\n",
    "        for data in alpaca_data:\n",
    "            instruction = f\"### Instruction: {data['instruction']}\"\n",
    "            input_text = f\"### Input: {data['input']}\"\n",
    "            response = f\"### Response: {data['output']}.\"\n",
    "            formatted_text = f\"{instruction}\\n{input_text}\\n{response}\\n\"\n",
    "            writer.writerow([formatted_text])\n",
    "\n",
    "# Usage\n",
    "alpaca_dataset_path = \"alpaca_data.json\"\n",
    "csv_output_path = \"test.csv\"\n",
    "preprocess_alpaca_json_to_csv(alpaca_dataset_path, csv_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Instruction: Give three tips for staying h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Instruction: What are the three primary co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Instruction: Describe the structure of an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Instruction: How can we reduce air polluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Instruction: Describe a time when you had ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ### Instruction: Give three tips for staying h...\n",
       "1  ### Instruction: What are the three primary co...\n",
       "2  ### Instruction: Describe the structure of an ...\n",
       "3  ### Instruction: How can we reduce air polluti...\n",
       "4  ### Instruction: Describe a time when you had ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "!apt-get install libjpeg-dev libpng-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 684, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\", line 50, in <module>\n",
      "    from .watermark import StableDiffusionXLWatermarker\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/watermark.py\", line 8, in <module>\n",
      "    from imwatermark import WatermarkEncoder\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/imwatermark/__init__.py\", line 1, in <module>\n",
      "    from .watermark import WatermarkEncoder, WatermarkDecoder\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/imwatermark/watermark.py\", line 5, in <module>\n",
      "    import cv2\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/cv2/__init__.py\", line 181, in <module>\n",
      "    bootstrap()\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/cv2/__init__.py\", line 153, in bootstrap\n",
      "    native_module = importlib.import_module(\"cv2\")\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "ImportError: libGL.so.1: cannot open shared object file: No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/bin/autotrain\", line 5, in <module>\n",
      "    from autotrain.cli.autotrain import main\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/autotrain/cli/autotrain.py\", line 6, in <module>\n",
      "    from .run_dreambooth import RunAutoTrainDreamboothCommand\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/autotrain/cli/run_dreambooth.py\", line 10, in <module>\n",
      "    from autotrain.trainers.dreambooth.__main__ import train as train_dreambooth\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/autotrain/trainers/dreambooth/__main__.py\", line 10, in <module>\n",
      "    from diffusers import StableDiffusionXLPipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 675, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 675, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 674, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/usr/local/python/3.10.8/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 686, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl because of the following error (look up to see its traceback):\n",
      "libGL.so.1: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#@title ðŸ¤— AutoTrain LLM\n",
    "#@markdown In order to use this colab\n",
    "#@markdown - upload train.csv to a folder named `data/`\n",
    "#@markdown - train.csv must contain a `text` column\n",
    "#@markdown - choose a project name if you wish\n",
    "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
    "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
    "#@markdown - update hyperparameters if you wish\n",
    "#@markdown - click `Runtime > Run all` or run each cell individually\n",
    "\n",
    "import os\n",
    "!pip install -U autotrain-advanced > install_logs.txt\n",
    "!autotrain setup --colab > setup_logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
    "project_name = 'my_autotrain_llm' # @param {type:\"string\"}\n",
    "model_name = 'abhishek/llama-2-7b-hf-small-shards' # @param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"\" #@param {type:\"string\"}\n",
    "repo_id = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 2e-4 # @param {type:\"number\"}\n",
    "num_epochs = 1 #@param {type:\"number\"}\n",
    "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "block_size = 1024 # @param {type:\"number\"}\n",
    "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
    "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
    "weight_decay = 0.01 # @param {type:\"number\"}\n",
    "gradient_accumulation = 4 # @param {type:\"number\"}\n",
    "use_fp16 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_int4 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "lora_r = 16 #@param {type:\"number\"}\n",
    "lora_alpha = 32 #@param {type:\"number\"}\n",
    "lora_dropout = 0.05 #@param {type:\"number\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
    "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
    "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"USE_PEFT\"] = str(use_peft)\n",
    "os.environ[\"USE_INT4\"] = str(use_int4)\n",
    "os.environ[\"LORA_R\"] = str(lora_r)\n",
    "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
    "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--data-path ./ \\\n",
    "--text-column text \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--epochs ${NUM_EPOCHS} \\\n",
    "--block-size ${BLOCK_SIZE} \\\n",
    "--warmup-ratio ${WARMUP_RATIO} \\\n",
    "--lora-r ${LORA_R} \\\n",
    "--lora-alpha ${LORA_ALPHA} \\\n",
    "--lora-dropout ${LORA_DROPOUT} \\\n",
    "--weight-decay ${WEIGHT_DECAY} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n",
    "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
    "$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--use-int4\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
